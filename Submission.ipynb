{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Team EEG Data\n",
    "Jungsu Pak, Rufei Fan, Alice Wong, Aaron Gonzales, Nathan Ong, Andrew Lee"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Data\n",
    "The cells below are for to you use to load the data in case they are similar to loading a specific series. Feel free to disregard them if you load the data another way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_task_eeg_series(datapath, sub, series):\n",
    "    # input arguments:\n",
    "    # datapath (string): path to the root directory\n",
    "    # sub (string): subject ID (e.g. subj1, subj2, etc)\n",
    "    # series (int): series name (e.g. 1, 2, etc). \n",
    "    # This will load in all of the specified data and chunk them by series\n",
    "    \n",
    "    # output:\n",
    "    # eegdata (numpy array): samples x channels data matrix\n",
    "    # eegevents (pandas dataframe): labels\n",
    "    # channel_names (list): names of the channels\n",
    "    import os\n",
    "    import pandas as pd\n",
    "    eegdata = pd.read_csv(os.path.join(datapath, sub + '_series' + str(series) + '_data.csv'))\n",
    "    eegevents = pd.read_csv(os.path.join(datapath, sub + '_series' + str(series) + '_events.csv'))\n",
    "    return eegdata.iloc[:].as_matrix(), eegevents, eegdata.keys()\n",
    "\n",
    "def load_task_eeg_data(datapath, sub, start, end):\n",
    "    # call this one in your code\n",
    "    # input arguments:\n",
    "    # datapath (string): path to the root directory\n",
    "    # sub (string): subject ID (e.g. subj1, subj2, etc)\n",
    "    # series (list): series number (e.g. [1,2,3] etc). \n",
    "    # This will load in all of the specified data and chunk them by series\n",
    "    \n",
    "    # output:\n",
    "    # eegdata (numpy array): samples x channels data matrix\n",
    "    # eegevents (pandas dataframe): labels\n",
    "    # channel_names (list): names of the channels\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    eegdata = []\n",
    "    eegevents = []\n",
    "    for s in range(start, end): # end is not inclusive\n",
    "        ed, ee, ek = load_task_eeg_series(datapath, sub, s)\n",
    "        ee['chunks'] = pd.Series(s * np.ones(ee.shape[0]))\n",
    "        eegdata.append(ed)\n",
    "        eegevents.append(ee)\n",
    "    eegkeys = ek    \n",
    "    return np.vstack(eegdata), pd.concat(eegevents), eegkeys\n",
    "\n",
    "def load_data(subj, train_start, train_end, test_start, test_end):\n",
    "    # subj requires a string (i.e. 'subj1')\n",
    "    # train_start, train_end is a range that specifies which series you want as training\n",
    "    #     (1, 7) means that the series from 1 to 6 (inclusive) are used for training\n",
    "    # test_start, test_end is also a range\n",
    "    #     (7, 9) means that the series from 7 to 8 (inclusive) are returned for testing\n",
    "    data_path = 'train' # modify this\n",
    "    train_data, train_event, tmp = load_task_eeg_data(data_path, subj, train_start, train_end)\n",
    "    test_data, test_event, tmp = load_task_eeg_data(data_path, subj, test_start, test_end)\n",
    "    return train_data, train_event, test_data, test_event"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How we processed our data\n",
    "This is case you want to use it. We used this for training our model. \n",
    "\n",
    "x is expected to be filtered and scaled (preprocessed) before calling this:\n",
    "\n",
    "```\n",
    "binary_shrink_data(x, y, nonlabel_factor=0.5, reduction_factor=3, shuffle_indices=False, verbose=True)\n",
    "```\n",
    "\n",
    "178803 rows with something and 1243589 rows with nothing.\n",
    "\n",
    "178803 rows with something and 177656 rows with nothing.\n",
    "\n",
    "Returning 71292 samples.\n",
    "\n",
    "\n",
    "What's happening it looks at the labels and count how many are 0s and 1s. In all series the number of samples with no label dominates the number of samples with some label. It shrinks the data by getting rid of samples with no labels until they the no-label samples account for `nonlabel_factor` % of the data. Then it naively downsamples the data by a factor of `reduction_factor`, by taking every n sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calc_reduction_ratio(a, b, factor):\n",
    "    # helper method\n",
    "    # it returns the factor to scale data to make them proportional by a factor of factor (var)\n",
    "    n = (a - factor * a) / float(factor * b)\n",
    "    return int(n)\n",
    "\n",
    "def reduce_samples_by_factor(x, y, factor):\n",
    "    # helper method that naively downsamples data without worrying about aliasing\n",
    "    return x[::factor], y[::factor]\n",
    "\n",
    "def binary_reduce_empty_samples(x, y, reduction_factor, shuffle_indices=False, verbose=False):\n",
    "    # x: np array, expects n x 32\n",
    "    # y: np array, expects n x 6\n",
    "    somethings = []\n",
    "    nothings = []\n",
    "    \n",
    "    new_y = np.zeros(y.shape[0], dtype=float)\n",
    "    \n",
    "    for i in range(y.shape[0]):\n",
    "        sum = np.sum(y[i, :])\n",
    "        if sum > 0:\n",
    "            somethings.append(i)\n",
    "            new_y[i] = 1.\n",
    "        else:\n",
    "            nothings.append(i)\n",
    "            new_y[i] = 0.\n",
    "            \n",
    "    if verbose:\n",
    "        print('%d rows with something and %d rows with nothing.' % (len(somethings), len(nothings)))\n",
    "        \n",
    "    factor = calc_reduction_ratio(len(nothings), len(somethings), reduction_factor)\n",
    "    \n",
    "    if shuffle_indices:\n",
    "        nothings = shuffle(nothings)\n",
    "\n",
    "    nothings = nothings[::factor] # take 1 out of every n to reduce the amount of nothings\n",
    "    \n",
    "    if verbose:\n",
    "        print('%d rows with something and %d rows with nothing.' % (len(somethings), len(nothings)))\n",
    "    \n",
    "    all_things = sorted(somethings + nothings)\n",
    "\n",
    "    return x[all_things], new_y[all_things]\n",
    "\n",
    "def binary_shrink_data(x, y, nonlabel_factor, reduction_factor, shuffle_indices=False, verbose=False):\n",
    "#     nonlabel_factor = 0.5 # nonlabel samples will account for % of the total data\n",
    "#     reduction_factor = 50 # divide number of samples by reduction factor\n",
    "    X_bal, y_bal = binary_reduce_empty_samples(x, y, nonlabel_factor, shuffle_indices, verbose)\n",
    "    X_bal, y_bal = reduce_samples_by_factor(X_bal, y_bal, reduction_factor)\n",
    "    if verbose:\n",
    "        print 'Returning %d samples.' % (X_bal.shape[0])\n",
    "    return X_bal, y_bal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "These are the methods used for preprocessing. You won't need to preprocess the data manually, although you will need to preprocess the labels it is not sure how you (Jeff) are loading them.\n",
    "\n",
    "During class today Uri said you are testing it by reducing the sampling frequency from 500 Hz to something smaller. You will need to do this in the `preprocess_data` method because if you do it before the filter you may get unexpected results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Preprocessing for skl models\n",
    "\n",
    "def apply_filters(x, order=4, fs=500.0, cutoff=50, axis=0):\n",
    "    # to use this just pass the data and use the existing data\n",
    "    # expects n x 32 in sequence\n",
    "    \n",
    "    # x: data, whole thing\n",
    "    # fs: frequency, 500\n",
    "    from scipy.signal import decimate, butter, filtfilt\n",
    "    nyq = .5 * fs\n",
    "    b, a = butter(order, cutoff/nyq, btype='low')\n",
    "    x = filtfilt(b, a, x, axis=axis)\n",
    "    return x\n",
    "\n",
    "def preprocess_data(X_orig, scaler):\n",
    "    # X_orig: eeg data, np array that is n x 32\n",
    "    # scaler for the subject\n",
    "    X_prep = scaler.transform(X_orig)\n",
    "    X_prep = apply_filters(X_prep)\n",
    "    \n",
    "    # May need to add code here\n",
    "    # if you are simply reducing the sampling frequency you can downsample by:\n",
    "    # X_prep = X_prep[::downsample_factor]\n",
    "    \n",
    "    return X_prep\n",
    "    \n",
    "def preprocess_labels(y):\n",
    "    # takes a n x 6 np array\n",
    "    # returns a n x 1 np array\n",
    "    new_y = np.zeros(y.shape[0], dtype=float)\n",
    "    \n",
    "    for i in range(y.shape[0]):\n",
    "        sum = np.sum(y[i, :])\n",
    "        if sum > 0:\n",
    "            new_y[i] = 1.\n",
    "        else:\n",
    "            new_y[i] = 0.\n",
    "    \n",
    "    return new_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Preprocessing for Keras model\n",
    "# No scaling used in it, just filtering with bandpass\n",
    "\n",
    "def butter_bandpass(lowcut, highcut, fs=500, order=5):\n",
    "    nyq = 0.5 * fs\n",
    "    low = lowcut / nyq\n",
    "    high = highcut / nyq\n",
    "    b, a = butter(order, [low, high], btype='band')\n",
    "    return b, a\n",
    "\n",
    "def butter_bandpass_filter(data, lowcut, highcut, fs=500, order=5):\n",
    "    b, a = butter_bandpass(lowcut, highcut, fs, order=order)\n",
    "    y = filtfilt(b, a, data, axis = 0)\n",
    "    return y\n",
    "\n",
    "def keras_preprocess_data(X_orig):\n",
    "    return butter_bandpass_filter(X_orig, 1, 50, 500, order=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run a Model\n",
    "\n",
    "## Skl Model\n",
    "Call `run_skl_model` using the raw data, it expects a n x 32 numpy array with data type being float64, although StandardScaler may automatically convert it for you. \n",
    "\n",
    "How to use: `run_skl_model(x, subj_id=1)` runs it for subject 1. It handles the preprocessing of the data for you.\n",
    "It will output a 1 if it expects any label in it. In our experience it shows a high amount of false positives, falsely classifying 0s as 1s.\n",
    "\n",
    "As mentioned in comments we only provided the models for subjects 1, 4, 7, and 10 for skl models.\n",
    "\n",
    "## Keras Model\n",
    "\n",
    "The Keras model is for subject 3. After you load your data for subject 3 call:\n",
    "\n",
    "```\n",
    "x = keras_preprocess_data(x_data)\n",
    "run_keras_model(x)\n",
    "```\n",
    "\n",
    "It will output either 0 or 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def run_skl_model(x, subj_id):\n",
    "    # subj_id: id of the subject\n",
    "    # we have models for subjects 1, 4, 7, & 10\n",
    "    # choosing an alternate will not load\n",
    "    # this preprocesses data for you but not labels\n",
    "    subj_str = 'subj%d' % (subj_id)\n",
    "    scaler_str = '%s_scaler.pkl' % (subj_str)\n",
    "    model_str = '%s_model.pkl' % (subj_str) \n",
    "    \n",
    "    scaler = joblib.load(scaler_str)\n",
    "    clf = joblib.load(model_str)\n",
    "    \n",
    "    proc_x = preprocess_data(x, scaler)\n",
    "\n",
    "    return clf.predict(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_keras_model(x):\n",
    "    # expects x to be preprocessed using keras_preprocess_data \n",
    "    from keras.models import load_model\n",
    "    model = load_model('KerasNNet.h5')  # load the saved keras model (and weights) so you don't have to retrain\n",
    "    prediction = model.predict_classes(x)\n",
    "    return prediction"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
